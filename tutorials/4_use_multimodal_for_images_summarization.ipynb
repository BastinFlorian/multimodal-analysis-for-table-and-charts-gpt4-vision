{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use multimodal for image summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how difficult it is to summarize an image using only text or only image. \n",
    "\n",
    "We will then show how to use multimodal to summarize an image using both text and image.\n",
    "\n",
    "We will use the image table used in the previous notebooks and a new chart:\n",
    "\n",
    "\n",
    "![\"Chart\"](../docs/images/figure9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_pdf_path = \"../data/pdf/graph.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract informations on theses images using Unstructured and Tesseract OCR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try to understand why we need the multimdoal usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "saved_image_directory_path = \"../data/pdf/extracted_images/figures/graph/\"\n",
    "\n",
    "graph_elements = partition_pdf(\n",
    "    filename=graph_pdf_path,\n",
    "    infer_table_structure=True,\n",
    "    strategy=\"hi_res\",\n",
    "    include_page_breaks=True,\n",
    "    chunking_strategy='auto',\n",
    "    extract_images_in_pdf=True,\n",
    "    extract_image_block_output_dir=saved_image_directory_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved in the saved_image_directory_path directory the detected images by OCR. \n",
    "\n",
    "Avec a look a the extracted image by this directory. \n",
    "\n",
    "You can see that the graph extraction is good but the informative title was not added to the image. \n",
    "\n",
    "Let's see the extracted text from the image and try to summarize the image using mutlimodal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Element --------\n",
      "Element type: <class 'unstructured.documents.elements.Title'>\n",
      "Element text: Large Language Model Context Size\n",
      "-------- Element --------\n",
      "Element type: <class 'unstructured.documents.elements.Image'>\n",
      "Element text: & | eos S Ss So So SF s oe gs z . é % we eg s ra ge * F s r - s fe & & Sg & & Pu Poa s Fs eg od 7 4 . - A a & & f . tha - a # ia - x o xy = 2 G OpenAI ANTHROP\\C —~——s F FB (P) Hugging Face o (7) <_ g = °§ 9 & o\n"
     ]
    }
   ],
   "source": [
    "from utils import pretty_print_element\n",
    "for element in graph_elements:\n",
    "    pretty_print_element(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The text extraction quality is not good at all. It cannot consiedered as useful for a question answerings RAG pupeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutlimodal summarization of graph images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from retrying import retry\n",
    "from langchain_community.chat_models import AzureChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# gpt4 vision preview only available in SWITZERLAND\n",
    "DEPLOYMENT_NAME_GPT4_VISION = 'gpt4-vision-switzerland'\n",
    "\n",
    "GPT_4_V = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT_GPT4_VISION\"),\n",
    "    openai_api_version=\"2023-07-01-preview\",\n",
    "    deployment_name=DEPLOYMENT_NAME_GPT4_VISION,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY_GPT4_VISION\"),\n",
    "    openai_api_type=\"azure\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=4000,\n",
    "    model_kwargs={\"top_p\": 0.95},\n",
    ")\n",
    "\n",
    "def encode_image(image_path: str) -> str:\n",
    "    \"\"\"Encode image to base64\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "@retry(stop_max_attempt_number=3, wait_fixed=60000)\n",
    "def summarize_image(encoded_image: str, prompt: str) -> str:\n",
    "    \"\"\"Apply batch image description from extracted text\"\"\"\n",
    "    return GPT_4_V.invoke(\n",
    "        input=[\n",
    "            SystemMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ]\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\",\n",
    "                                    \"detail\": \"high\"\n",
    "                                },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    ).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "figure_filepaths =  [os.path.join(saved_image_directory_path,  filename) for filename in os.listdir(saved_image_directory_path)]\n",
    "\n",
    "PROMPT_TABLE_AND_IMAGE_SUMMARIZATION = \"\"\"\n",
    "    You are an AI assistant that summarize images containing charts\n",
    "    For each chart in the image:\n",
    "    - Summarize the chart as a table\n",
    "    Also, provide a title of each extracted table or image and a two paragraphs summary.\n",
    "    Provide range values interval or approximations if needed.\n",
    "    Add two '\\n\\n' between each table or chart description\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Comparison of Parameters in Various AI Models\n",
      "\n",
      "| AI Model               | Number of Parameters (Approx.) |\n",
      "|------------------------|--------------------------------|\n",
      "| bloom                  | 0                              |\n",
      "| GPT-NeoX-20B           | 0                              |\n",
      "| Gopher-280B            | 0                              |\n",
      "| PaLM-540B              | 0                              |\n",
      "| Wu Dao 2.0             | 0                              |\n",
      "| GPT-3                  | 0                              |\n",
      "| LaMDA                  | 0                              |\n",
      "| GPT-4                  | 0                              |\n",
      "| MT-NLG                 | 0                              |\n",
      "| Chinchilla             | 0                              |\n",
      "| PanGu-Alpha            | 0                              |\n",
      "| GPT-NeoX               | 0                              |\n",
      "| GPT-4                  | 0                              |\n",
      "| text-davinci-003       | 0                              |\n",
      "| text-davinci-002       | 0                              |\n",
      "| text-curie-001         | 0                              |\n",
      "| text-babbage-001       | 0                              |\n",
      "| text-ada-001           | 0                              |\n",
      "| LaMDA 137B             | 0                              |\n",
      "| LaMDA 137B-2           | 0                              |\n",
      "| commercial-nlp         | 0                              |\n",
      "| text-davinci-003-turbo | 0                              |\n",
      "| text-curie-001-turbo   | 0                              |\n",
      "| text-babbage-001-turbo | 0                              |\n",
      "| text-ada-001-turbo     | 0                              |\n",
      "| GPT-4                  | 0                              |\n",
      "| GPT-3.5-turbo          | 0                              |\n",
      "| GPT-3.5-turbo-64k      | 0                              |\n",
      "| GPT-3.5-turbo-32k      | 0                              |\n",
      "| GPT-3.5-turbo-8k       | 0                              |\n",
      "| GPT-3.5-turbo-4k       | 0                              |\n",
      "| GPT-3.5-turbo-2k       | 0                              |\n",
      "| GPT-3.5-turbo-1k       | 0                              |\n",
      "| Claude                 | 0                              |\n",
      "| Claude-1               | 0                              |\n",
      "| Claude-2               | 10000                          |\n",
      "\n",
      "The chart presents a comparison of various AI models based on the number of parameters they contain. The models are listed along the horizontal axis, with their corresponding number of parameters represented by the vertical bars. The majority of the models have a parameter count close to zero, indicating that they are not as complex or perhaps not as large in scale as the model with the highest parameter count. The chart includes models from different organizations such as Hugging Face, OpenAI, Meta AI, Cohere, and Anthropic, as indicated by the logos beneath the model names.\n",
      "\n",
      "The standout model in this chart is 'Claude-2' from Anthropic, which has a significantly higher number of parameters, approximately 10,000, compared to the rest. This suggests that 'Claude-2' is a much larger and potentially more powerful AI model in terms of its capacity to process and analyze data. The chart does not provide specific numbers for the other models, but it is clear that 'Claude-2' is an outlier in this comparison, indicating a possible focus on developing a highly advanced AI system by Anthropic.\n"
     ]
    }
   ],
   "source": [
    "for filepath in figure_filepaths:\n",
    "    encoded_image = encode_image(filepath)\n",
    "    print(summarize_image(encoded_image, prompt=PROMPT_TABLE_AND_IMAGE_SUMMARIZATION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The context around an image is very important to understand the image.\n",
    "\n",
    "Without a title, on this non informative graph, GPT 4 Vision was not able to summarize the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize at the page level using multimodal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unstructured saved the found images with the following format: \n",
    "\n",
    "`figure-[page_number]-[image_number].jpg`\n",
    "\n",
    "We can then identify the page number from our PDF containing images. \n",
    "\n",
    "Let's collect each page as an image ans ask GPT 4 Vision to summarize the page_numbers with image in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/pdf/extracted_images/figures/graph/figure-1-1.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(figure_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/pdf/extracted_images/pages/graph/page_1.jpg\n",
      "Title: Large Language Model Context Size Summary Table\n",
      "\n",
      "| Model Name              | Context Size (Tokens) |\n",
      "|-------------------------|-----------------------|\n",
      "| gpt-neo-125M            | Approx. 2,000         |\n",
      "| gpt-neo-1.3B            | Approx. 2,000         |\n",
      "| gpt-neo-2.7B            | Approx. 2,000         |\n",
      "| gpt-j-6B                | Approx. 4,000         |\n",
      "| EleutherAI/gpt-neo-2.7B | Approx. 4,000         |\n",
      "| gpt3-ada                | Approx. 4,000         |\n",
      "| gpt3-babbage            | Approx. 4,000         |\n",
      "| gpt3-curie              | Approx. 4,000         |\n",
      "| gpt3-davinci            | Approx. 4,000         |\n",
      "| EleutherAI/gpt-neo-1.3B | Approx. 4,000         |\n",
      "| EleutherAI/gpt-neo-125M | Approx. 4,000         |\n",
      "| gpt-neox-20b            | Approx. 8,000         |\n",
      "| gpt-j-6B                | Approx. 8,000         |\n",
      "| t5-11b                  | Approx. 8,000         |\n",
      "| t5-3b                   | Approx. 8,000         |\n",
      "| gpt3.5-turbo-1.3B       | Approx. 8,000         |\n",
      "| gpt3.5-turbo-3B         | Approx. 8,000         |\n",
      "| gpt3.5-turbo-6B         | Approx. 8,000         |\n",
      "| gpt3.5-turbo-13B        | Approx. 8,000         |\n",
      "| gpt3.5-turbo-175B       | Approx. 8,000         |\n",
      "| Claude                  | Approx. 8,000         |\n",
      "| Claude-2                | Approx. 100,000       |\n",
      "\n",
      "Summary:\n",
      "\n",
      "The image depicts a bar chart titled \"Large Language Model Context Size,\" which compares the context sizes of various language models in terms of tokens. The context size refers to the number of tokens that the model can consider at once when generating or processing text. The chart shows a range of models from different organizations such as Hugging Face, OpenAI, Meta AI, and Anthropic, with context sizes ranging from approximately 2,000 tokens to 100,000 tokens.\n",
      "\n",
      "The smallest context sizes are around 2,000 tokens for models like gpt-neo-125M, gpt-neo-1.3B, and gpt-neo-2.7B. As we move towards larger models, we see an increase in context size, with several models like gpt3-ada, gpt3-babbage, gpt3-curie, and gpt3-davinci having a context size of approximately 4,000 tokens. The largest context size in the chart is for the model Claude-2 by Anthropic, which stands out significantly with an approximate context size of 100,000 tokens, indicating its capability to consider a much larger context when processing language compared to other models listed.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "image_filepath: str = f'../data/pdf/extracted_images/pages/graph/'\n",
    "page_numbers_with_images: List[int] = list(set([int(filepath.split('/')[-1].split('-')[1]) for filepath in figure_filepaths]))\n",
    "images = convert_from_path(graph_pdf_path)\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    image.save(os.path.join(image_filepath, f\"page_{i+1}.jpg\")) # Unstructured page count starts from 1\n",
    "\n",
    "for page_number in page_numbers_with_images:\n",
    "    image_path = os.path.join(image_filepath, f\"page_{i+1}.jpg\")\n",
    "    print(image_path)\n",
    "    encoded_image = encode_image(image_path)\n",
    "    print(summarize_image(encoded_image, prompt=PROMPT_TABLE_AND_IMAGE_SUMMARIZATION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
