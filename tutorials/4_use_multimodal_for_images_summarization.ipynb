{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use multimodal for image summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how difficult it is to summarize an image using only text or only image. \n",
    "\n",
    "We will then show how to use multimodal to summarize an image using both text and image.\n",
    "\n",
    "We will use the image table used in the previous notebooks and a new chart:\n",
    "\n",
    "\n",
    "![\"Chart\"](../docs/images/figure9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import base64\n",
    "from typing import List\n",
    "from retrying import retry\n",
    "from pdf2image import convert_from_path\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain_community.chat_models import AzureChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_pdf_path = \"../data/pdf/graph.pdf\"\n",
    "saved_image_directory_path = \"../data/pdf/extracted_images/figures/graph/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract informations on theses images using Unstructured and Tesseract OCR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try to understand why we need the multimdoal usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "graph_elements = partition_pdf(\n",
    "    filename=graph_pdf_path,\n",
    "    infer_table_structure=True,\n",
    "    strategy=\"hi_res\",\n",
    "    include_page_breaks=True,\n",
    "    chunking_strategy='auto',\n",
    "    extract_images_in_pdf=True,\n",
    "    extract_image_block_output_dir=saved_image_directory_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved in the saved_image_directory_path directory the detected images by OCR. \n",
    "\n",
    "Avec a look a the extracted image by this directory. \n",
    "\n",
    "You can see that the graph extraction is good but the informative title was not added to the image. \n",
    "\n",
    "Let's see the extracted text from the image and try to summarize the image using mutlimodal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Element --------\n",
      "Element type: <class 'unstructured.documents.elements.Title'>\n",
      "Element text: Large Language Model Context Size\n",
      "-------- Element --------\n",
      "Element type: <class 'unstructured.documents.elements.Image'>\n",
      "Element text: & | eos S Ss So So SF s oe gs z . é % we eg s ra ge * F s r - s fe & & Sg & & Pu Poa s Fs eg od 7 4 . - A a & & f . tha - a # ia - x o xy = 2 G OpenAI ANTHROP\\C —~——s F FB (P) Hugging Face o (7) <_ g = °§ 9 & o\n"
     ]
    }
   ],
   "source": [
    "from utils import pretty_print_element\n",
    "for element in graph_elements:\n",
    "    pretty_print_element(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The text extraction quality is not good at all. It cannot consiedered as useful for a question answerings RAG pupeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutlimodal summarization of graph images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/florian.bastin/miniconda3/envs/.multi_venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.azure_openai.AzureChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# gpt4 vision preview only available in SWITZERLAND\n",
    "DEPLOYMENT_NAME_GPT4_VISION = 'gpt4-vision-switzerland'\n",
    "\n",
    "GPT_4_V = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT_GPT4_VISION\"),\n",
    "    openai_api_version=\"2023-07-01-preview\",\n",
    "    deployment_name=DEPLOYMENT_NAME_GPT4_VISION,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY_GPT4_VISION\"),\n",
    "    openai_api_type=\"azure\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=4000,\n",
    "    model_kwargs={\"top_p\": 0.95},\n",
    ")\n",
    "\n",
    "def encode_image(image_path: str) -> str:\n",
    "    \"\"\"Encode image to base64\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "@retry(stop_max_attempt_number=3, wait_fixed=60000)\n",
    "def summarize_image(encoded_image: str, prompt: str) -> str:\n",
    "    \"\"\"Apply batch image description from extracted text\"\"\"\n",
    "    return GPT_4_V.invoke(\n",
    "        input=[\n",
    "            SystemMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ]\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\",\n",
    "                                    \"detail\": \"high\"\n",
    "                                },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    ).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_filepaths =  [os.path.join(saved_image_directory_path,  filename) for filename in os.listdir(saved_image_directory_path)]\n",
    "\n",
    "PROMPT_TABLE_AND_IMAGE_SUMMARIZATION = \"\"\"\n",
    "    You are an AI assistant that summarize images containing charts\n",
    "    For each chart in the image:\n",
    "    - Summarize the chart as a table\n",
    "    Also, provide a title of each extracted table or image and a two paragraphs summary.\n",
    "    Provide range values interval or approximations if needed.\n",
    "    Add two '\\n\\n' between each table or chart description\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: AI Model Parameter Count Comparison\n",
      "\n",
      "| AI Model                 | Parameter Count (Approx.) |\n",
      "|--------------------------|---------------------------|\n",
      "| GPT-3 175B              | 175,000                   |\n",
      "| GPT-3 13B               | 13,000                    |\n",
      "| GPT-3 6.7B              | 6,700                     |\n",
      "| GPT-3 2.7B              | 2,700                     |\n",
      "| GPT-3 1.3B              | 1,300                     |\n",
      "| GPT-3 355M              | 355                       |\n",
      "| GPT-3 125M              | 125                       |\n",
      "| GPT-2 1.5B              | 1,500                     |\n",
      "| GPT-2 774M              | 774                       |\n",
      "| GPT-2 355M              | 355                       |\n",
      "| GPT-2 117M              | 117                       |\n",
      "| GPT-NeoX 20B            | 20,000                    |\n",
      "| GPT-Neo 2.7B            | 2,700                     |\n",
      "| GPT-Neo 1.3B            | 1,300                     |\n",
      "| GPT-J 6B                | 6,000                     |\n",
      "| GPT-NeoX-20B            | 20,000                    |\n",
      "| GPT-4 175B              | 175,000                   |\n",
      "| GPT-4 13B               | 13,000                    |\n",
      "| GPT-4 6.7B              | 6,700                     |\n",
      "| GPT-4 2.7B              | 2,700                     |\n",
      "| GPT-4 1.3B              | 1,300                     |\n",
      "| GPT-4 355M              | 355                       |\n",
      "| GPT-4 125M              | 125                       |\n",
      "| Claude 1                | 7,000                     |\n",
      "| Claude 2                | 10,000                    |\n",
      "\n",
      "The chart presents a comparison of various AI models based on the number of parameters they contain. The models range from GPT-3 to GPT-4 and include Claude models as well. The parameter count is a rough indicator of the complexity and potential capabilities of each model, with higher numbers generally suggesting a more sophisticated model capable of understanding and generating more nuanced human-like text.\n",
      "\n",
      "The models are listed in ascending order of parameter count, starting with the smallest GPT-3 125M with approximately 125 million parameters, and ending with the largest GPT-4 175B with approximately 175 billion parameters. Notably, the chart includes multiple versions of GPT-3 and GPT-4, highlighting the different scales at which these models operate. The Claude models are the newest additions with Claude 1 and Claude 2 having around 7 billion and 10 billion parameters respectively, indicating a significant leap in model complexity compared to some of the earlier versions of GPT-3.\n"
     ]
    }
   ],
   "source": [
    "for filepath in figure_filepaths:\n",
    "    encoded_image = encode_image(filepath)\n",
    "    print(summarize_image(encoded_image, prompt=PROMPT_TABLE_AND_IMAGE_SUMMARIZATION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The context around an image is very important to understand the image.\n",
    "\n",
    "Without a title, on this non informative graph, GPT 4 Vision was not able to summarize the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize at the page level using multimodal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unstructured saved the found images with the following format: \n",
    "\n",
    "`figure-[page_number]-[image_number].jpg`\n",
    "\n",
    "We can then identify the page number from our PDF containing images. \n",
    "\n",
    "Let's collect each page as an image ans ask GPT 4 Vision to summarize the page_numbers with image in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/pdf/extracted_images/figures/graph/figure-1-1.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(figure_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/pdf/extracted_images/pages/graph/page_1.jpg\n",
      "Title: Large Language Model Context Size\n",
      "\n",
      "| Model Name                | Context Size (Tokens) |\n",
      "|---------------------------|-----------------------|\n",
      "| bloom                     | 0 - 5,000             |\n",
      "| bloom-560m                | 0 - 5,000             |\n",
      "| bloom-1.1b-7-pytorch      | 0 - 5,000             |\n",
      "| gpt-neo-1.3b              | 0 - 5,000             |\n",
      "| gpt-neox-20b              | 0 - 5,000             |\n",
      "| gpt-j                    | 0 - 5,000             |\n",
      "| gpt3-6.7b                 | 0 - 5,000             |\n",
      "| gpt3-13b                  | 0 - 5,000             |\n",
      "| gpt3-175b                 | 0 - 5,000             |\n",
      "| jurassic-1-jumbo          | 0 - 5,000             |\n",
      "| gpt4-32k                  | 0 - 5,000             |\n",
      "| gpt4-64k                  | 0 - 5,000             |\n",
      "| gpt4-80k                  | 0 - 5,000             |\n",
      "| gpt3.5-turbo              | 0 - 5,000             |\n",
      "| gpt3-2.7b                 | 0 - 5,000             |\n",
      "| gpt3-6.7b                 | 0 - 5,000             |\n",
      "| gpt3-13b                  | 0 - 5,000             |\n",
      "| gpt3-175b                 | 0 - 5,000             |\n",
      "| Claude                    | 0 - 5,000             |\n",
      "| Claude 2.0                | 80,000 - 100,000      |\n",
      "\n",
      "Summary:\n",
      "The chart presents the context size of various large language models, with a focus on the number of tokens they can handle. Most of the models listed, including bloom, gpt-neo-1.3b, gpt-neox-20b, gpt-j, and various versions of gpt3, have a context size within the range of 0 to 5,000 tokens. This indicates a standardization or common capacity among many large language models in terms of the amount of text they can process in one go.\n",
      "\n",
      "However, there are notable exceptions at the higher end of the scale, specifically with the Claude and Claude 2.0 models. These models have a significantly larger context size, with Claude 2.0 reaching between 80,000 to 100,000 tokens. This substantial increase in context size for Claude 2.0 suggests a major advancement in the model's ability to process and understand much larger bodies of text, which could have implications for the complexity and depth of tasks it can perform.\n"
     ]
    }
   ],
   "source": [
    "image_filepath: str = f'../data/pdf/extracted_images/pages/graph/'\n",
    "page_numbers_with_images: List[int] = list(set([int(filepath.split('/')[-1].split('-')[1]) for filepath in figure_filepaths]))\n",
    "images = convert_from_path(graph_pdf_path)\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    image.save(os.path.join(image_filepath, f\"page_{i+1}.jpg\")) # Unstructured page count starts from 1\n",
    "\n",
    "for page_number in page_numbers_with_images:\n",
    "    image_path = os.path.join(image_filepath, f\"page_{i+1}.jpg\")\n",
    "    print(image_path)\n",
    "    encoded_image = encode_image(image_path)\n",
    "    print(summarize_image(encoded_image, prompt=PROMPT_TABLE_AND_IMAGE_SUMMARIZATION))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multim_venv",
   "language": "python",
   "name": "multim_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
